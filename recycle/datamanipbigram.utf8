zheng et
yields accuracy
yielding data
yet general
xie et
wu et
written as
write as
worth noting
work wu
work we
work upon
work tan
work ren
work kobayashi
work jiang
work is
work has
work each
work by
words to
words the
words replacing
words predicted
words in
words from
words after
wordnet miller
word with
word substitutions
word substitution
word striking
without changing
with varying
with various
with variety
with their
with the
with synonyms
with rφ
with reinforcement
with pytorch
with pretrained
with policy
with parameters
with or
with new
with model
with log
with learning
with large
with imbalanced
with imagenet
with gtx
with factor
with eq
with data
with bland
with being
with base
with an
with above
will be
widely used
while weighting
while the
while on
while imdb
while for
which we
which uses
which powerful
which our
which optimizes
which makes
which learns
which is
which includes
which in
which fills
which creates
which can
which are
where φi
where φ0
where various
where the
where since
where only
where is
when updating
when sampling
when sample
when minibatch
when it
when is
were implemented
well on
well in
well due
weights using
weights this
weights respectively
weights ren
weights our
weights of
weights more
weights in
weights by
weights based
weights as
weights are
weights adaptively
weighting with
weighting which
weighting we
weighting to
weighting that
weighting scheme
weighting results
weighting respectively
weighting provides
weighting performs
weighting on
weighting network
weighting methods
weighting method
weighting mechanism
weighting is
weighting in
weighting has
weighting for
weighting data
weighting by
weighting boost
weighting assigns
weighting approach
weighting and
weighted update
weighted sampling
weight value
weight to
weight ren
weight learning
weight associated
we use
we update
we train
we thus
we subsample
we study
we speculate
we similarly
we showcase
we show
we select
we report
we replace
we randomly
we propose
we present
we pick
we optimize
we obtain
we observe
we now
we next
we learn
we instead
we instantiate
we have
we further
we formulate
we follow
we first
we evaluate
we draw
we discuss
we devise
we demonstrate
we could
we conduct
we compare
we choose
we can
we automate
we augment
we are
we approximate
we also
we aim
ways the
ways resulting
way natural
was derived
visually striking
vision simard
vision and
via meta
very small
very limited
very helpful
very few
verify our
varying reward
varying imbalance
varying data
vary the
various ways
various settings
various rule
various problems
various manipulation
various algorithms
variety of
variational policy
variational distribution
vanilla maximum
values when
values freund
value with
value we
value of
validation sets
validation set
validation instances
validation examples
validation data
validation accuracy
validating that
valid reward
val data
usually designed
usual and
using wordnet
using validation
using the
uses validation
uses held
uses fixed
useful samples
used we
used to
used throughout
used proportion
used in
used here
used generative
used for
used especially
use the
use slightly
use sgd
use resnet
use reinforcement
use only
use of
use new
use large
use gumbel
use efficient
use both
use 10
upon which
upon recent
upon acceptance
updating the
updates our
updated with
updated to
updated throughout
updated instead
update the
update is
update for
update depends
until convergence
unlabeled data
unit reward
uniform way
uniform distribution
under and
uncased model
unaware of
ultimate performance
ubiquitous to
typically focused
types than
types of
types li
type we
type of
type data
two ways
two types
two tokens
two settings
two samples
two particular
two parameterizations
two paradigms
two masked
two manipulation
two distinct
two difficult
two classes
tuning we
tuning the
tuning text
tunes the
tunes an
tuned augmentation
tune the
true label
tried fitting
trec respectively
trec is
trec has
trec for
trec 40
treats the
treats data
treatment is
treatment can
treatment achieves
treating data
treated as
translation sennrich
transforms the
transformer or
transformations on
transformation network
transformation functions
tran et
training we
training validation
training significantly
training signals
training sets
training set
training samples
training sample
training proceeds
training previous
training of
training loss
training it
training instead
training instances
training for
training examples
training example
training epoch
training different
training data
training and
trained to
trained on
trained models
train the
train on
train data
total while
too small
tokens striking
to when
to vision
to use
to train
to through
to this
to the
to sub
to specialized
to significantly
to show
to see
to respective
to relax
to preserve
to predict
to perform
to over
to our
to optimize
to maximum
to maximize
to manipulate
to make
to learn
to just
to its
to investigate
to instantiate
to induce
to improve
to hinder
to generate
to fit
to explore
to expand
to excel
to eq
to enable
to empirically
to each
to diverse
to different
to cubuk
to create
to connect
to build
to both
to boost
to bert
to be
to avoid
to automatically
to augment
to assign
to any
to additionally
to adapt
thus we
thus the
thus omitted
thus favorable
thus can
thus beneficial
thus adapt
throughout the
through novel
through finetuning
through eq
through efficient
three datasets
though the
though more
though having
this work
this section
this perspective
this is
this can
these learning
these algorithms
there is
there are
then written
then used
then to
then fixed
then fed
then adding
their synonyms
their gradient
the word
the weights
the weighted
the weight
the varying
the vanilla
the value
the validation
the use
the update
the ultimate
the two
the true
the training
the top
the three
the text
the task
the target
the supplementary
the sum
the strong
the strict
the step
the state
the sst
the softmax
the small
the size
the single
the shelf
the sentence
the selected
the second
the samples
the same
the reward
the results
the resulting
the respective
the resnet
the relevant
the recent
the rare
the proportion
the process
the problem
the previous
the pretrained
the predicted
the practical
the popular
the policy
the parametric
the parameter
the paper
the output
the original
the objective
the numbers
the number
the notations
the normalization
the next
the new
the mstep
the most
the models
the model
the minibatch
the method
the maximum
the masked
the marriage
the manipulation
the low
the log
the lm
the literature
the latest
the large
the lack
the labels
the kullback
the joint
the ith
the intrinsic
the instantiation
the input
the initialization
the improvement
the imbalanced
the image
the highest
the heuristic
the gradient
the generality
the future
the function
the full
the fixed
the first
the expressive
the expected
the expectation
the exact
the established
the equivalence
the equation
the engineering
the end
the empirical
the distance
the dataset
the data
the conventional
the context
the conditioning
the computation
the composition
the commonly
the combination
the classification
the class
the cifar10
the cases
the boxes
the best
the bert
the base
the average
the augmented
the augmentation
the augmentatin
the art
the approach
the amount
the algorithms
the algorithm
the advantage
the actual
the above
the 50
that were
that weights
that very
that treats
that the
that takes
that supports
that some
that samples
that replaces
that our
that optimizes
that once
that learns
that keeps
that is
that instantiate
that has
that fine
that even
that enables
that dynamically
that different
that data
that augmentation
that are
that additive
than weighting
than ren
than eq
than data
text transformation
text samples
text data
text contextual
text classification
text augmentation
text and
tested the
test set
terms of
term with
tends to
tend to
technique than
tasks for
task reward
task or
task has
task datasets
task data
task by
target model
tan et
taking the
takes action
table that
table shows
table also
table accuracy
systems has
synthesis with
synthesis which
synthesis in
synthesis generates
synonyms using
synonyms and
synonym approach
synonym 32
supports learning
supplementary materials
supervised setting
supervised learning
superiority over
superior to
superior results
superior performance
summarized in
sum of
suitable technique
suitable for
such extremely
such as
success such
substitutions that
substitutions predicted
substitutions of
substitutions and
substitution words
substitution the
substitution section
substitution for
substitution and
subsets for
subset of
subsample small
sub optimal
stunning is
study we
study the
study on
study different
study conventional
studied various
studied in
studied automated
strong restrictions
strong pretrained
strong base
striking with
striking in
striking and
strict matching
straightforward to
stochastic optimization
stochastic gradient
still be
steps which
steps selected
steps our
steps fails
step we
step using
step is
states during
state of
state let
state further
standard deviation
staged it
sst with
sst the
sst task
sst sentiment
sst imbalanced
sst for
sst and
sst 40
speech kobayashi
speech ko
speculate that
specified approaches
specifically we
specifically the
specifically on
specifically given
specifically from
specifically for
specific types
specific manipulation
specialized reward
special instance
sometimes harms
sometimes harm
some previous
solved with
solid arrows
softmax φi
softmax over
softmax normalization
socher et
so that
smoothly and
small validation
small training
small thus
small subset
small size
small set
small fitting
small data
small amount
slightly more
slightly adapted
size the
size data
single algorithm
since the
since text
since our
since every
since as
simple yet
similarly our
similarly create
simard et
significantly improves
significantly improved
significantly improve
significant challenges
signals are
shrivastava et
shows the
shows that
shows superiority
shows equivalence
showing the
showing that
showcase data
show two
show the
show significantly
show in
show improved
shelf reward
shannon entropy
sgd optimization
setup we
setup for
settings with
settings there
settings of
settings in
setting where
setting the
setting specifically
setting given
setting for
sets the
sets for
sets as
sets are
set we
set validation
set to
set our
set of
set instead
set gradient
set formally
set for
set enriched
set and
sequence generation
sentiment socher
sentiment maas
sentence specifically
sentence sentiment
sentence label
sentence is
sentence class
sentence and
sennrich et
self paced
selecting steps
selected subsets
selected in
select the
select class
select 000
seems to
see the
see our
see from
section we
section due
section as
second is
scratch previous
scratch in
scratch at
scopes our
schemes with
schemes we
schemes the
schemes for
schemes are
schemes also
scheme alternatively
schapire 1997
scale unlabeled
scale pretraining
scale pretrained
scale corpus
sampling substitution
samples weights
samples to
samples so
samples from
samples for
samples are
samples and
sample weights
sample receives
sample not
sample matches
sample importance
sample by
same processing
same number
same algorithm
rφ which
rφ the
rφ that
rφ instead
rφ in
rφ can
rφ as
rδ with
rδ in
rδ equals
rδ dv
runs one
rules do
rule or
rule and
row shows
roth 2002
rotation krizhevsky
rl for
rl and
rich types
rich line
rewards through
rewards and
reward with
reward when
reward under
reward the
reward that
reward rφ
reward parameter
reward only
reward learning
reward is
reward into
reward in
reward function
reward free
reward extrinsic
reward even
reward eq
reward enriched
reward can
reward and
reward achieved
review sentiment
return varying
return valid
retrofitted as
results table
results our
results on
results of
results moreover
results indeed
results however
results by
results are
resulting model
resulting in
resulting algorithms
resulting algorithm
restrictions of
restricted to
respectively the
respectively lastly
respectively in
respectively comparing
respective comparison
respective colors
respective classes
resnets we
resnet he
resnet 34
research on
reported results
report the
replacing φi
replacing word
replacing the
replaces words
replace rδ
ren et
relevant work
relevance to
relaxing the
relaxed reward
relax the
related work
reinforcement learning
regularized by
regime or
regime and
regardless of
reformulating maximum
reduced to
red arrows
recent work
recent connection
receives unit
receive unit
reason we
re estimating
re estimates
re estimated
ratner et
ratios we
rate of
rare classes
ranging from
randomly select
randomly picking
randomly or
randomly masks
randomly initialized
random guess
ram all
question types
quality of
quality datasets
pθ which
pθ where
pθ variational
pθ regularized
pθ and
pytorch pytorch
pytorch org
published upon
providing training
provides the
provides only
provide historical
proposed approach
propose new
proportion method
proportion based
proportion 72
proportion 57
propagation through
processing on
process by
proceeds through
procedure is
problems of
problems here
problem where
problem setting
problem is
probable word
probable substitution
probabilities by
prior uniform
principle adopt
previous work
previous weight
previous methods
previous manipulation
pretraining data
pretrained weights
pretrained on
pretrained not
pretrained networks
pretrained language
pretrained column
preserving transformations
preserve the
present the
present slightly
present machine
present learning
presence of
predicted words
predicted with
predicted by
predict sample
practice when
practical implementation
powerful pretrained
potentially help
positions with
pose significant
popular benchmark
policy where
policy we
policy that
policy parameter
policy optimization
policy is
policy gradient
points to
points on
points each
point in
plugging rw
plugging in
picking 40
pick an
perturbs examples
perspective of
perspective due
perspective data
performs better
performs best
performed on
performance sst
performance over
performance on
performance of
performance measure
performance it
performance in
performance as
performance and
perform well
per class
peng et
pass dashed
pass and
particular the
particular study
particular relevance
particular parameterizations
particular manipulation
particular interest
particular instead
part or
park et
parentheses next
parametric treatment
parametric intrinsic
parameters to
parameters that
parameters text
parameters of
parameters maintained
parameters intuitively
parameterizing rφ
parameterized in
parameterizations other
parameterizations of
parameterizations for
parameterization of
parameterization and
parameter updates
parameter since
parameter learning
parameter is
parameter in
parameter effectively
parameter and
paradigms results
paradigm of
paced learning
over weighting
over the
over strong
over 20
over 15
output learned
output for
outperforms the
outperform variety
out validation
out set
out few
ours fine
ours 75
ours 38
ours 36
our work
our weight
our proposed
our method
our instantiated
our data
our context
our base
our automated
our augmentation
our approach
our algorithm
our adaptive
others sennrich
others chang
other than
other comparison
other cases
osindero 2014
original words
original task
original sentence
original framework
original data
org and
or with
or weighting
or substitutions
or replacing
or loss
or learning
or is
or in
or generator
or datasets
or augmenting
optimizes intrinsic
optimizes and
optimize the
optimize on
optimize in
optimize by
optimization with
optimization procedure
optimization perspective
optimization is
optimization framework
optimization for
optimization consider
optimal results
operators with
only when
only very
only the
only limited
only in
one standard
once we
on we
on various
on validation
on training
on thus
on their
on the
on text
on sst
on particular
on original
on model
on linux
on learning
on large
on imdb
on imbalanced
on images
on imagenet
on image
on enriched
on end
on each
on different
on both
on binary
on automated
on all
on 20
omitted here
omitted as
off the
of words
of we
of very
of validation
of treating
of training
of the
of text
of target
of supervised
of steps
of rφ
of rδ
of reward
of research
of reformulating
of re
of previous
of policy
of particular
of our
of only
of notations
of noise
of namely
of most
of model
of meta
of manipulations
of manipulation
of low
of learning
of is
of interest
of held
of extrinsic
of existing
of examples
of epoch
of enabling
of each
of diverse
of different
of data
of constant
of class
of cifar10
of both
of augmentation
of around
of and
of adaptively
of adaptive
of 4e
of 1e
obtain the
observed that
observed in
observe that
observe significant
observation further
objective on
objective of
objective maximizes
objective is
objective here
numbers in
number of
now reduced
now develop
now demonstrate
novel marriage
novel contextual
notpretrained denotes
noting that
note that
notations with
notations used
not pretrained
not observe
not in
not fit
normalization term
normalization in
noisy augmented
noise during
next to
next study
next step
next section
new words
new sample
new relaxed
new method
new instances
new approach
neural networks
networks such
networks for
networks as
network to
network jointly
network fixed
network by
network and
negative infinite
natural objective
natural idea
names indicates
mstep maximizes
movie review
most probable
most present
moreover in
more validation
more types
more suitable
more smoothly
more imbalanced
more details
more detailed
more concretely
more coherent
more carefully
more broadly
modern machine
models we
models such
models for
models even
models are
models and
model we
model val
model training
model trained
model to
model through
model this
model states
model sst
model since
model selection
model seems
model resnet
model pθ
model pretrained
model performs
model performance
model parameters
model parameter
model on
model of
model lm
model is
model increases
model in
model for
model devlin
model can
model by
model bert
model being
model based
model and
model 20
mirza and
mirroring rotation
mining shrivastava
minimizes the
minibatch stochastic
minibatch examples
minibatch by
miller 1995
methods we
methods that
methods specifically
methods respectively
methods on
methods are
method we
method that
method sometimes
method showing
method outperforms
method of
method is
method in
method first
method consistently
meta learning
merged training
mechanism has
mechanism can
measure of
measure in
maximum likelihood
maximizing the
maximizing data
maximizes the
maximize the
materials and
matching condition
matches training
matches different
matches data
masks out
masked tokens
masked sentence
masked positions
masked for
marriage of
manually specified
manually augmenting
manipulations and
manipulation types
manipulation type
manipulation the
manipulation that
manipulation tend
manipulation solid
manipulation schemes
manipulation rφ
manipulation reward
manipulation parameters
manipulation parameter
manipulation on
manipulation methods
manipulation learning
manipulation is
manipulation involves
manipulation in
manipulation have
manipulation has
manipulation function
manipulation formulation
manipulation for
manipulation either
manipulation can
manipulation as
manipulation are
manipulation approach
manipulation and
manipulating small
manipulating data
manipulate data
malisiewicz et
makes label
makes data
make the
make substitutions
make better
maintained throughout
maintained and
machine with
machine learning
maas et
low quality
low data
lots of
loss recent
log φi
log pθ0
log pθ
log likelihood
lm to
lm such
lm parameter
lm on
lm jointly
lm is
lm instead
lm for
lm eq
lm data
lm augmentation
literature in
literature compared
list the
linux machine
line of
limited improvement
limited data
limited application
likelihood on
likelihood of
likelihood objective
likelihood learning
li and
leveraged validation
let rin
let pθ
let dv
let denote
less studied
lemley et
leibler divergence
learns text
learns separate
learns augmentation
learning with
learning weighting
learning we
learning there
learning the
learning systems
learning section
learning rl
learning ren
learning ratner
learning rate
learning pipelines
learning of
learning model
learning manipulation
learning kumar
learning is
learning for
learning especially
learning different
learning deep
learning data
learning consistently
learning compared
learning clearer
learning by
learning based
learning as
learning and
learning algorithm
learned model
learned jointly
learned in
learn the
learn different
learn data
lead to
latest weighting
latest model
lastly we
largely define
large scale
language model
lack of
labels per
labels is
labels for
labels faithfully
labels code
labels both
labels and
labeled examples
label preserving
label preservation
label denote
label conditional
label and
kumar et
kullback leibler
krizhevsky et
kobayashi 2018
ko et
kl is
key difference
keeps the
katharopoulos and
just different
jointly with
joint training
joint rewards
joint manipulation
joint learning
joint distribution
joint data
jiang et
jang et
its simplicity
its fine
its effect
ith example
iteratively updated
iteration we
iteration the
iteration ren
iteration experiments
iteration based
it matches
it is
it has
it also
is zheng
is worth
is widely
is we
is used
is updated
is unaware
is to
is thus
is then
is the
is that
is summarized
is straightforward
is state
is solved
is set
is sample
is rich
is retrofitted
is ren
is reduced
is randomly
is prior
is omitted
is now
is negative
is interesting
is inspired
is function
is first
is enough
is derived
is degenerated
is configuration
is built
is better
is beneficial
is backpropagated
is applicable
is an
is also
is again
is achieved
involves data
investigate the
inverse class
intuitively the
introduces lots
introduced an
introduce reward
intrinsic rewards
intrinsic reward
into eq
interestingly the
interesting to
interesting perspective
interest to
interest as
instead the
instead of
instead formulate
instantiations augmentation
instantiation of
instantiating the
instantiates different
instantiated to
instantiated data
instantiate the
instantiate our
instantiate distinct
instantiate and
instances per
instances of
instances in
instances has
instances for
instances and
instance to
instance of
instance focuses
inspired from
inspiration from
input the
input and
initializing the
initialized with
initialized randomly
initialized as
initialize model
initial learning
information which
infinite in
inducing data
inducing curriculum
induces sample
induce the
indicates the
indeed the
increasingly used
increasingly ubiquitous
increases as
including sst
includes 40
included in
include cropping
inaccurate and
in which
in vision
in various
in uniform
in two
in total
in this
in the
in that
in text
in terms
in supervised
in simple
in sentence
in section
in rφ
in respective
in ren
in reinforcement
in principle
in previous
in presence
in practice
in particular
in part
in parentheses
in our
in noisy
in more
in modern
in minibatch
in low
in kobayashi
in is
in instead
in figure
in eq
in epoch
in effect
in each
in different
in contrast
in analogue
in all
in algorithm
in 20
improves the
improves over
improvement showing
improvement over
improved performance
improved learning
improve the
improve over
improve model
impressive results
importance weight
importance experiments
implemented with
implementation of
imdb has
imdb for
imdb as
imdb and
imdb 40
imbalanced sst
imbalanced ranging
imbalanced labels
imbalanced data
imbalanced classification
imbalanced cifar10
imbalance ratios
imbalance problems
images include
imagenet we
imagenet pretrained
image we
image or
image data
image classification
illustrates the
if the
if gφ
idea of
however the
historical information
hinder model
highest probabilities
heuristicbased methods
heuristic rules
heuristic based
here we
hence the
helpful to
help with
held out
he et
having been
have limited
have different
have developed
have been
have alleviated
has used
has typically
has the
has studied
has developed
has crucially
has been
has become
has also
has 90
has 40
has 270
has 210
harms the
harm the
hard example
gumbel softmax
guess this
gtx 1080ti
grey are
grey antiseptic
greatly improves
gradient through
gradient propagation
gradient is
gradient for
gradient directions
gradient descent
gradient based
gradient all
gpus and
given the
given set
given an
giridhara et
gets more
generator respectively
generation problems
generation can
generating useful
generates entire
generate substitutions
generality for
general algorithm
gans baluja
further shows
further introduce
further create
functions instead
function we
function since
function rφ
function reward
function of
function instantiates
function in
function and
full test
from validation
from the
from table
from scratch
from distinct
from cifar10
from around
freund and
frequency or
free parameters
framework tan
framework it
framework in
forth recent
formulation we
formulation of
formulation for
formulate manipulation
formally let
for word
for validation
for training
for the
for text
for substitution
for striking
for sst
for specific
for selecting
for resnets
for parameterization
for our
for manipulation
for manipulating
for learning
for label
for joint
for inducing
for imdb
for images
for image
for fixed
for fair
for example
for each
for different
for data
for consistency
for class
for certain
for both
for binary
for automated
for augmentation
for adaptive
following wu
follow ren
focuses on
focused on
fleuret 2018
fixed steps
fixed proportion
fixed our
fixed number
fixed conditional
fixed bert
fixed augmentation
fixed and
fitting the
fit to
fit the
fischer 2017
first update
first row
first randomly
first present
first method
first is
first fit
first example
finetuning the
fine tuning
fine tunes
fine tuned
fine tune
fills the
figure words
figure the
figure illustrates
figure algorithm
few words
few labeled
favorable as
fan et
faithfully resulting
fair comparison
fails to
factor of
extrinsic reward
extrinsic and
extremely small
extensive experiments
expressive formulation
explore the
explore more
exploding value
experiments were
experiments to
experiments that
experiments show
experiments of
experiments are
experimental setup
expected reward
expected extrinsic
expectation of
expand the
exp rφ
exp rδ
existing rule
excited to
excel in
examples without
examples with
examples we
examples the
examples providing
examples or
examples on
examples labels
examples in
examples different
examples are
example where
example to
example rφ
example replacing
example plugging
example only
example mining
example learns
example in
example from
example data
example 40
exact training
exact original
ex in
ex gφ
evolves and
every class
even with
even when
even though
evaluated on
evaluate two
evaluate the
et al
estimating sample
estimates sample
estimated from
establishes the
established framework
especially in
equivalence to
equivalence between
equation shows
equals the
equal contribution
eq αkl
eq we
eq vanilla
eq respectively
eq as
eq and
eq 12
eq 11
epoch we
epoch charming
epoch and
ep exp
ep eq
entropy of
entropy and
entire new
entire artificial
enriched with
enriched objective
enough to
enhance the
engineering burden
end tasks
end natural
enables learning
enable efficient
empirically verify
empirical data
emotionally desiccated
em algorithm
either augmentation
efficient stochastic
efficient gradient
efficiency specifically
effectively uses
effect on
easily by
each with
each training
each task
each step
each outperform
each of
each minibatch
each iteration
each instance
each has
each dataset
each data
each class
dynamically re
dynamically adjusts
dynamically adapts
dv be
during the
during augmentation
due to
draws inspiration
draw inspiration
domains common
domain that
does not
do not
diverse manipulation
diverse data
divergence is
distribution the
distribution kl
distribution hence
distribution exp
distribution as
distinct paradigm
distinct methods
distinct data
distance between
discuss in
discrete to
directly be
directly backpropagate
directions we
directions on
difficult settings
different types
different schemes
different problem
different perspective
different parameterization
different manipulation
different from
different data
different contexts
different classes
different application
different applicable
difference in
difference following
did not
devlin et
devise method
deviation the
developed novel
developed new
developed automated
develop our
details in
detailed in
designed specifically
designed manually
designed for
descent to
descent on
deriving sample
derives data
derived for
derived based
depends on
dependent reward
depended on
denotes the
denotes 40
denote the
denote learning
denote forward
denote backward
demonstrate two
demonstrate the
demonstrate in
degenerated to
define sample
deep neural
decaying the
datasets well
datasets to
datasets pose
datasets including
datasets imbalanced
datasets besides
datasets are
dataset while
dataset we
dataset names
data which
data weights
data weighting
data we
data val
data used
data transformer
data transformation
data to
data the
data synthesis
data such
data size
data setting
data reward
data results
data regime
data points
data point
data our
data of
data manipulation
data log
data interestingly
data instances
data in
data gets
data for
data examples
data example
data during
data domains
data domain
data distribution
data dependent
data cubuk
data available
data augmented
data augmentation
data and
data 64
data 52
data 38
data 35
dashed arrows
curriculum jiang
cubuk et
crucially depended
cropping mirroring
creates entire
create small
create augmented
could in
could have
corresponding to
corpus and
conventional synonym
conventional approach
contrast the
contrast notpretrained
contrast learning
contextual augmentation
contexts augmentation
context of
constant by
consistently outperforms
consistently improves
consistently better
consistency of
consider the
connection of
connect the
configuration can
conduct extensive
conditioning label
conditional model
conditional generation
conditional bert
condition of
concretely in
computation flow
composition of
comparison methods
comparison approaches
comparing the
compared to
compare with
compare our
commonly used
common heuristicbased
common data
combination of
column is
colors list
cold grey
coherent with
code and
clearer note
classifier model
classification we
classification the
classification tasks
classification specifically
classification similarly
classification results
classification performance
classification on
classification for
classification can
classification augmenting
classification all
classes the
classes are
class we
class the
class since
class question
class is
class instances
class imbalance
class has
class frequency
class for
class and
cifar10 used
cifar10 the
cifar10 data
cifar10 classification
choose strong
changing the
chang et
challenges for
certain types
cases where
cases to
case study
carefully designed
cancels the
can write
can still
can sometimes
can see
can return
can receive
can potentially
can lead
can have
can fail
can easily
can directly
can be
can adapt
by the
by taking
by rδ
by relaxing
by randomly
by parameterizing
by our
by noting
by maximizing
by learning
by inverse
by instantiating
by inducing
by generating
by flipping
by decaying
by additionally
burden and
built this
builds upon
build the
broadly applicable
boxes in
both the
both text
both of
both methods
both manipulation
both datasets
both augmentation
boost the
boost base
blue arrows
bland in
binary movie
binary classification
between the
between data
between and
better use
better than
better at
best step
best on
best across
besides our
besides compared
bert which
bert lm
bert language
bert devlin
bert classifier
bert base
beneficial to
beneficial the
beneficial in
benchmark socher
benchmark datasets
being initialized
being data
behavior as
been used
been pretrained
been less
been increasingly
become increasingly
because the
be very
be the
be suitable
be sentence
be published
be policy
be parameterized
be more
be learned
be instantiated
be beneficial
be because
be applied
be achieved
based weighting
based synonym
based reward
based on
based method
based data
based augmentation
based approaches
based algorithm
base uncased
base models
base model
baluja and
balancing parameters
backpropagated to
backpropagate the
back translation
avoid exploding
averaged over
average number
available while
available both
automatically fine
automated data
automated augmentation
automate the
automate any
augmenting with
augmenting only
augmenting by
augmented update
augmented samples
augmentation wu
augmentation which
augmentation weighting
augmentation we
augmentation though
augmentation that
augmentation tends
augmentation shows
augmentation performs
augmentation our
augmentation or
augmentation operators
augmentation network
augmentation model
augmentation method
augmentation mechanism
augmentation lm
augmentation language
augmentation kobayashi
augmentation has
augmentation for
augmentation distribution
augmentation cubuk
augmentation can
augmentation behavior
augmentation as
augmentation approaches
augmentation approach
augmentation applies
augmentation and
augmentation 37
augment each
augment data
at each
at addressing
associated with
assigns an
assign an
as with
as weighting
as we
as usual
as the
as special
as reward
as pθ
as policy
as parameters
as our
as manipulation
as learning
as label
as it
as is
as in
as gφ0
as gans
as function
as contextual
as case
as bert
as also
as adaboost
as above
artificial samples
artificial examples
art approach
arrows denote
around accuracy
around 50
argmaxφ ep
argmaxφ dv
argmaxθ ex
argmaxθ ep
are usually
are treated
are too
are then
are superior
are small
are reduced
are parameters
are more
are learned
are iteratively
are initialized
are included
are imbalanced
are excited
are evaluated
are equivalent
are discrete
are balancing
are averaged
are available
are alternative
approximation jang
approximate the
approaches xie
approaches such
approaches in
approaches have
approaches designed
approaches cubuk
approach with
approach uses
approach treats
approach that
approach significantly
approach ren
approach provides
approach of
approach make
approach is
approach including
approach greatly
approach for
approach draws
approach does
approach can
approach by
approach builds
approach besides
approach adapts
applies label
applied to
application settings
application scope
applicable to
applicable scopes
any off
any manipulation
any data
another type
andreas 2019
and yields
and will
and were
and weighting
and vary
and variational
and validation
and used
and use
and updated
and two
and trec
and trained
and thus
and the
and text
and target
and speech
and so
and slickly
and show
and select
and schapire
and sample
and roth
and reward
and respectively
and resnet
and ren
and reinforcement
and pθ
and provide
and produced
and previous
and parameterizing
and parameter
and others
and osindero
and obtain
and model
and minimizes
and meta
and manually
and manipulation
and learning
and its
and is
and introduces
and intrinsic
and instantiation
and instances
and in
and imdb
and imbalanced
and image
and grey
and from
and fleuret
and fit
and fischer
and explore
and eq
and epoch
and enable
and emotionally
and efficiency
and dynamically
and did
and data
and class
and are
and and
and alternatingly
and also
and adapts
and 64gb
and 100
analysis benchmark
analogue to
an original
an off
an interesting
an initial
an importance
an image
an example
an entire
an em
an augmentation
an action
amount of
amount and
although visually
alternatively rφ
alternative parameterizations
alternatingly corresponding
also tried
also tested
also study
also shows
also observed
also observe
also more
also leveraged
also establishes
also each
also cold
also achieved
alleviated the
all the
all settings
all results
all reported
all other
all experiments
algorithms such
algorithms significantly
algorithms largely
algorithms are
algorithm zheng
algorithm where
algorithm to
algorithm that
algorithm our
algorithm optimizes
algorithm next
algorithm joint
algorithm is
algorithm in
algorithm from
algorithm for
algorithm figure
algorithm differs
algorithm computation
algorithm can
al domains
aim to
again consistently
after training
advantage that
advantage of
adopt any
adjusts the
addressing class
additive transforms
additionally fine
adding the
adapts the
adapts an
adaptively generating
adaptively from
adaptive data
adaptive augmentation
adapted formulation
adapt its
adapt an
adam optimization
adaboost freund
actual parameterization
action given
action and
across all
achieves superior
achieved in
achieved impressive
achieved easily
achieved by
achieve 100
accuracy points
accuracy of
accuracy if
above update
above two
above reward
above optimization
above observation
above it
above gradient
above formally
above data