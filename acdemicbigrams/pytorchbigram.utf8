10 and
10 of
100 is
11 turned
112 192
113 218
119 212
12 scipy
12 torch
123 112
128 10
13 and
14 in
14 made
14 torch
1422 27
15 219
15 9631
15 made
1547 316
15512 4e6
1554 22
16 package
17 15512
17 of
19 and
192 557
1960s the
20 this
200 216
2017 in
2019 submissions
2019 vancouver
21 in
212 463
216 15
218 444
22 113
22 along
23 and
24 produced
25 or
26 given
2698 v4
27 66
28 consequently
29 format
296 iclr
30 pytorch
31 32
316 119
32bit floats
33 effectively
33 python
33rd conference
34 hasktorch
35 and
36 or
38 to
39 40
40 41
41 to
43 given
44 to
463 17
50 and
50 mobilenet
50 model
50 on
50 the
512 bytes
557 24
66 200
778 15
84 210
845 84
933 123
9631 8e6
__getitem__ the
__init__ self
__init__ super
__len__ the
ability of
able to
academic work
accelerated by
acceleration and
accelerators such
acceptable 100
accepts added
account for
achieve almost
achieve compelling
achieve high
achieves competitive
across range
activation similarly
activations self
activations t2
actually extremely
adam discriminator
adam generator
adapt to
adapts to
add support
added by
addition to
additional features
additional optimization
additional synchronization
additionally providing
address new
advances in
advent of
again that
ai therefore
algorithms efficiently
all aspects
all designed
all else
all expressed
all gpu
all gpus
all memory
all mentions
all of
all previously
all reduce
all work
alleviate this
allocate an
allocated on
allocated regions
allocation is
allocation was
allocations end
allocations to
allocations without
allocator because
allocator can
allocator starts
allocator was
allocator which
allocators pytorch
allow concurrent
allow for
allow pytorch
allow researchers
allow users
allowed our
allowed us
allowing for
allowing them
allows changing
allows for
allows pytorch
allows the
allows us
allows users
almost every
almost perfect
along with
already utilize
also allowed
also available
also crucial
also execute
also integrates
also intend
also provides
although not
always use
among arxiv
among the
among them
among these
amortized performance
an allocation
an array
an efficient
an entire
an example
an explosion
an input
an interesting
an interpreted
an iterator
an optimization
an output
an overall
analysis needs
analysis plotting
and 24
and __len__
and advances
and all
and applies
and automatically
and backward
and basic
and commoditization
and consistent
and cpu
and cublas
and cudafree
and design
and differentiable
and distributed
and does
and efficient
and ensure
and extensibility
and external
and find
and flux
and future
and hard
and hidden
and how
and implement
and implementing
and improves
and in
and increasing
and initialize
and is
and julia
and keep
and lush
and made
and makes
and management
and many
and mobilenet
and modeling
and moves
and numpy
and one
and optimized
and optimizers
and others
and pandas
and parallelizing
and pragmatic
and production
and productive
and provides
and pythonic
and reassigns
and record
and recursive
and releasing
and result
and run
and scalability
and similar
and softmax
and some
and supporting
and synchronization
and take
and tensorflow
and theano
and their
and then
and they
and towards
and two
and uncommon
and unexpected
and whether
and which
and whose
annotated traces
another additional
another interesting
another unique
antipattern among
any component
any data
any given
any new
any number
any particular
apis free
apis the
apis would
apl matlab
applied repeatedly
applies to
approach but
approach is
approach it
approach provides
approach the
approach to
approach which
approaches are
approaches while
arbitrary programs
arbitrary python
architecture can
architecture we
are actually
are available
are benign
are correct
are designed
are exceptions
are familiar
are four
are free
are generated
are in
are just
are mentioned
are not
are of
are python
are reflected
are summarized
are typically
are unacceptable
are used
are usually
are working
areas are
arithmetic complexity
around three
array level
arrays are
arrays hence
arrays often
arrows pair
arxiv papers
arxiv prints
as 23
as 2d
as an
as apl
as building
as chainer
as classes
as cudnn
as expected
as long
as lua
as matlab
as model
as numpy
as part
as performance
as possible
as proxy
as pythonic
as scarce
as stateful
as tensors
aspects of
assumption simplifies
asynchronous dataflow
asynchronously execute
at any
attribute this
authoring by
autograd 16
autograd function
automatically because
automatically moves
automatically the
autonomously playing
availability and
available during
available throughout
available to
average arithmetic
avoid bad
avoid fragmentation
avoid introducing
avoid this
aware of
away from
backward methods
bad interactions
balances speed
based approach
based around
based deep
based on
based programming
basic building
basic inter
basic parallel
batch normalization
batches of
batching parallelization
be able
be achieved
be added
be arbitrary
be completely
be created
be executed
be extensible
be first
be further
be leveraged
be observed
be pythonic
be represented
be run
be understood
be useful
be very
because exact
because taking
because the
become increasingly
become popular
become unneeded
been an
before they
behave like
behavior for
behavior of
behaviors at
behind intuitive
behind our
being closely
being equal
benchmarks and
benchmarks the
beneficial for
benefit of
benefits of
benign and
better 26
better interoperability
better to
between its
between numpy
bidirectional exchange
bindings and
block for
block its
blocking the
blocks of
body of
both cases
both pytorch
both references
both sides
bottom row
branches loops
brand new
building block
building blocks
builds on
built into
but complete
but do
but later
but let
but not
but slightly
but the
but this
but we
by blocking
by carefully
by chainer
by combining
by comparing
by composing
by comprehensive
by counting
by deep
by developing
by extending
by gpus
by integrating
by its
by keeping
by leveraging
by our
by python
by pytorch
by quantifying
by subclassing
bytes to
cache of
caching tensor
caffe chainer
caffe cntk
caffe in
caller until
calls to
can define
can differentiate
can easily
can get
can only
can reallocate
can rely
can then
can theoretically
care not
care of
careful and
careful implementation
careful performance
carefully insert
carefully optimizing
case insensitive
case is
cases it
cases result
cases without
caveat is
centric design
certain corner
chainer 778
chainer cntk
chainer dynet
chainer or
challenging for
changing most
changing needs
characteristics in
cheap and
child processes
choices dynamic
chose to
class bindings
class fullbasicmodel
class linearlayer
class member
classes that
classes whose
classify gray
closed proprietary
closely integrated
closely resembles
cntk 845
cntk keras
cntk mxnet
cntk tensorflow
code as
code by
code describing
code in
code most
code on
code will
collection is
collector built
colored areas
combination has
combine multiple
combining focus
comes at
commoditization of
common antipattern
common benchmarks
common deep
common for
common technique
common visualization
commonly established
commonly used
communication and
communication channel
communication primitives
communities that
community by
community has
community moved
community received
community to
community with
comparable to
compare the
comparing it
compatible it
competitive performance
compilation before
complete neural
completely interchangeable
completely up
completes to
complex and
complexity in
complexity inherent
complexity of
complexity pytorch
complicated cases
component of
components of
compose individual
composing functionality
comprehensive but
comprehensive set
computation and
computation by
computation of
computation that
computations can
computations to
computations with
compute gradients
computed function
computes the
computing derivatives
computing have
computing libraries
computing power
computing utilities
concepts developed
conclusion and
conference on
conforming to
consequently the
considerations in
consistent ideally
consistent with
constant time
construct static
construction of
constructors create
consumes objects
containing number
continue to
continuing to
control and
control flow
control of
control program
control the
controlled in
conv nn
conv2d 128
converged on
convert between
converted arrays
convolution batch
convolution matrix
cooperatively share
copies and
copies have
copy on
copying objects
core libtorch
core pytorch
corner cases
correspond to
corresponding execution
costs of
could leverage
could quickly
could utilize
count reaches
counted tools
counting cpython
counting how
counting mechanism
counting scheme
course part
cpu before
cpu gpu
cpu however
cpu immediately
cpu operators
cpu or
cpu runs
cpu scheduling
cpu thread
cpu to
cpu which
cpu with
cpus and
cpython swift
create and
create bindings
create first
created by
critical systems
critical to
cross thread
crucial for
cublas libraries
cuda apis
cuda kernel
cuda runtime
cuda tensors
cudafree routine
cudafree slow
cudamalloc and
cudnn 22
cudnn and
cumbersome instead
current implementation
custom allocator
custom caching
custom differentiable
custom layer
data dataset
data files
data of
data parallelism
data processing
data running
data scientists
data stored
data structure
data this
data we
data which
data with
dataflow graph
dataflow on
dataloader class
dataset preprocessing
datasets behave
datasets can
daunting labor
dealing with
deallocation it
debuggers and
debugging easy
decisions and
def __init__
def step
default implementation
define new
defined behavior
defining layers
deliver compelling
deliver great
deliver that
demonstrate the
depend on
depends on
depicts the
derivative or
derivatives of
derivatives this
describe how
describing pytorch
design also
design and
design assumption
design choices
design decisions
design employed
design goals
design greatly
design is
design principles
design process
design seems
design that
design their
design was
designed specifically
desired performance
despite being
detach fake_label
detail the
details all
developed for
developing and
device operators
device utilization
different approach
different machine
differentiable functions
differentiable via
differentiate through
differentiating functions
differentiation 15
differentiation and
differentiation but
differentiation cumbersome
differentiation integrated
differentiation using
differentiation which
differently by
differs significantly
digits 27
disappears in
discriminator and
discriminator create_discriminator
discriminator parameters
discriminator real_sample
distinct pool
distributed computation
distributed computing
dlpack 29
do so
do that
does so
domain specific
done on
down the
down this
dramatically by
drop in
dropout and
drove the
due to
during one
during training
dynamic memory
dynamic programming
dynamic tensor
dynamically allocate
dynet lush
dynet pytorch
dynet which
e5 2698
each model
each month
eager mode
eager semantics
eagerly all
easier to
easily adapts
easily extended
easily implemented
easily leverage
easily spawn
easy to
eblearn 21
ecosystem it
ecosystem most
ecosystem of
ecosystem such
effect disappears
effect of
effectively ensures
effectiveness this
effects and
effects of
efficiency of
efficient and
efficient core
efficient gradient
efficient interoperability
efficiently executed
efficiently from
eigen 13
either already
either at
either on
either usability
elements in
else being
else however
emphasize that
employing mutation
empower them
empowering its
enabled frameworks
enabled python
enables them
end up
engine 37
engine for
engineering resources
ensure safety
entire model
entirely in
enumerates used
equal the
errd_fake backward
errd_fake loss
errd_real backward
errd_real loss
errg backward
errg loss
error which
errors go
essential feature
essential skill
evaluation of
evaluator which
even in
even possible
events in
every cuda
every operator
everything else
everything is
exact ratio
exact scheduling
exactly when
example gpu
example implementation
example to
exceptions to
exchange data
exchange happens
exchange of
execute dataflow
execute eagerly
execute in
executed by
executed entirely
executed in
executed on
executed outside
executed using
execution can
execution if
execution quite
execution takes
execution while
exhibits unwanted
existing functionality
expect one
expected users
expense of
experiment with
experimentation of
experiments were
explain how
explicit triggers
explosion of
expressed as
expressive faster
extended to
extending the
extends the
extends this
extensible for
external libraries
external references
extremely cheap
facilitates the
fact compatible
fact that
fact torch7
fairly high
fake detach
fake generator
fake real_label
familiar concepts
familiar with
fast pace
faster language
fastest current
fastest framework
fastest speed
fc linearlayer
fc t1
features adapt
features of
feed forward
few operations
few operators
field of
fifo this
figure among
figure annotated
figure shows
figure the
figure trace
figure we
files an
finally the
finally we
find performance
find that
find their
finishes since
first calls
first iteration
first pytorch
first starting
fixed amount
floating point
floats throughput
flow graph
flow is
flow running
flux jl
focused implementation
focused on
follows the
for assignment
for better
for bidirectional
for custom
for distributed
for each
for efficient
for every
for general
for lengthy
for long
for machine
for models
for most
for neural
for numpy
for on
for pytorch
for significantly
for simple
for starcraft
for tensors
for user
for various
forces the
foregoes the
form of
formally the
format note
formulas for
forward layers
forward methods
forward mode
forwardmode automatic
fostered the
four main
four major
fragmented per
framework chainer
framework throughput
framework we
frameworks based
frameworks cntk
frameworks converged
frameworks have
frameworks implement
frameworks percentage
frameworks we
free of
free precedes
free to
freed region
frees everything
frees the
from python
from recognizing
from simple
from that
from utilizing
from weaving
fulfilled most
full control
fullbasicmodel nn
fully automate
function every
function that
functional relu
functional softmax
functionality is
functionality of
functionality provided
functions composed
functions cudamalloc
functions that
functions this
functions to
further improve
further use
future garbage
future we
future work
game engine
garbage collection
garbage collector
generated using
generator and
generator create_generator
generator get_noise
generator parameters
get an
gil python
given fixed
given paper
given the
given time
gnmtv2 model
gnmtv2 ncf
go away
go down
goals of
gp100 gpu
gpu acceleration
gpu and
gpu as
gpu by
gpu enabled
gpu finishes
gpu so
gpu synchronization
gpu we
gpus but
gpus completes
gpus hardware
gpus provided
gradient formulas
gradient of
gradients of
graph based
graph metaprogramming
graph sidestep
graph that
gray areas
gray scale
great care
great performance
greatly improves
grew from
growing complexity
guarantee the
handle dataset
handled automatically
handled by
handles sharing
happens in
hardware fifo
hardware such
has become
has been
has established
has good
has recognized
has to
has turned
hasktorch 35
have become
have implemented
have often
have simple
heavily parallel
hence lowering
hence pytorch
hence since
hence those
hence while
hidden behind
high overhead
higher is
hogwild 42
hold the
holding the
hoping that
how an
how it
how large
how model
how simple
how the
how these
how they
how to
how well
however by
however if
however it
however on
however python
however with
iclr 2019
ideally with
ideas into
idiomatic way
if an
if the
images note
images per
immediate execution
immediately as
immediately once
impact on
imperative and
imperative programs
imperative style
implement basic
implement heavily
implement techniques
implement the
implement this
implementation 30
implementation accepts
implementation does
implementation to
implementations of
implementations that
implemented versioning
implementer and
implementing two
implements custom
implements forward
implements the
implicit parameters
important for
importantly intermediate
importantly users
impose any
improve its
improve performance
improve support
improve the
improvements independent
improves performance
improves the
in addition
in an
in bold
in both
in caffe
in each
in figure
in frameworks
in functions
in general
in given
in images
in implementations
in languages
in linear
in lisp
in listing
in modern
in multithreaded
in objective
in order
in package
in parallel
in places
in practical
in profiler
in projects
in pytorch
in recent
in replacement
in scientific
in subsequent
in surprisingly
in table
in that
in their
in time
in to
in tokens
in user
in way
including caffe
including the
incomplete solution
inconvenient such
increased interest
increasing batch
increasingly important
incredibly varied
incremental allocation
incrementally builds
independent gpus
independent of
indexing operator
individual digits
individual layers
individual subsystems
inefficient when
information processing
inherent to
initial release
initialize their
inputs is
insensitive to
insert additional
instance 296
instance layers
instead of
instead pytorch
instrument various
integrates naturally
integrating with
intel xeon
intend to
inter process
interactions with
interest in
interesting and
interesting side
interesting tradeoff
interface and
interface as
interfaces simple
intermediate computations
internal implementation
internal to
internally by
interoperability because
interoperability is
interpret memory
interpreted language
interpreter is
interpreter the
interpreter where
into autonomously
into design
into first
into incredibly
into lua
into the
into torch
introduces pytorch
introducing subtle
intuitive apis
investigates the
invisible to
invocations on
invocations to
is acceptable
is common
is completely
is consistent
is designed
is drop
is dynamic
is even
is freed
is hard
is hardware
is inconvenient
is inefficient
is just
is less
is machine
is measured
is more
is nearly
is needed
is regular
is released
is rich
is running
is shared
is shown
is susceptible
is the
is therefore
is used
is usually
is vital
is within
is written
isolation weaker
issues moreover
it achieves
it almost
it an
it can
it causes
it easy
it follows
it has
it maintains
it needs
it opens
it over
it possible
it relies
it rounds
it significantly
it transparently
it will
it without
iteration differs
iteration of
iterations as
iterator over
its architecture
its caller
its control
its current
its derivative
its effectiveness
its existence
its implementation
its interpreter
its last
its programming
its results
its runtime
its user
its users
january 2017
jax 17
jit suite
jl 18
julia 11
jumpstart one
just program
just python
just the
keep up
keeping interfaces
keeping the
keras mxnet
kernel invocations
kernel writers
kernels launched
key components
knowing how
labor of
language its
language that
language torch
language with
languages other
languages resulting
languages that
large arrays
large the
largely without
last use
last used
later allocations
later synchronize
launched during
layer used
layers are
layers but
layers composing
layers into
layers which
lazy lists
learning algorithms
learning and
learning applications
learning approaches
learning community
learning for
learning grew
learning kernels
learning libraries
learning methods
learning models
learning pytorch
learning research
learning this
learning workflows
length operator
lengthy compilation
less common
less expressive
let them
lets them
level dual
leverage additional
leverage other
leverage similar
leverage the
leveraged to
leveraging the
libraries 39
libraries and
libraries as
libraries such
libraries that
libraries while
libraries with
library but
library for
library implements
library or
library provides
like copy
like hogwild
like nimtorch
like numpy
like possibly
like python
likely want
limited to
limiting since
limits their
linear layers
linear sequence
linearlayer 128
linearlayer module
lisp and
lisp torch
listing custom
listing demonstrates
listing simplified
lists how
loaders and
loaders as
loading and
loading data
loading the
lock gil
long as
long periods
longer than
loss functions
lua eblearn
machine eager
made array
made by
made the
main principles
maintain design
maintaining performance
maintains distinct
maintains strict
major trends
make writing
makes debugging
makes the
making ahead
making datasets
making it
manage carefully
manage tensor
management functions
management of
management on
manually control
many loops
many of
many popular
many scripting
many users
massively parallel
mathematical primitives
matlab and
matplotlib all
matrix multiplication
may block
measure as
measured in
mechanism 38
mechanism is
mechanism this
meet the
member of
memory ahead
memory allocator
memory allocators
memory as
memory available
memory errors
memory for
memory freed
memory immediately
memory overall
memory region
memory these
memory to
memory usage
memory without
mention common
mention pytorch
mentioned on
mentioning pytorch
mentions among
mentions of
meta data
metaprogramming based
method similar
methods __getitem__
methods process
methods specialized
methods which
mm activations
mobilenet gnmtv2
mobilenet models
mode automatic
mode differentiation
mode performance
model accelerated
model as
model authoring
model can
model makes
model of
model parallelism
model which
model works
modeling libraries
models and
models at
models data
models in
models loading
models specified
models the
models to
models using
modern machine
modifications and
module class
module containing
module into
monolithic kernels
month that
monthly number
more closely
more efficiently
more formally
more importantly
more memory
more moreover
more outputs
moreover it
moreover many
moreover the
most behaviors
most built
most deep
most importantly
most mutations
most notably
moved away
movement the
moves rust
moves the
multi stream
multidimensional arrays
multiple other
multiple streams
multiple tasks
multiple times
multiples of
multiplication dropout
multiprocessing which
multivariate input
must be
must dynamically
mutation on
mutations are
mxnet 1554
mxnet and
mxnet pytorch
naturally with
ncf model
nearly invisible
needed by
needed to
needs by
needs of
needs or
negate the
network architecture
network effects
networks in
networks one
networks themselves
neural information
neurips 2019
never exhibits
never uses
new allocation
new datasets
new ones
new or
new potential
new situations
new subclass
new training
nimtorch 34
nn conv2d
nn module
no copies
no matter
normalization and
not allow
not at
not aware
not both
not have
not limited
not meet
not pypy
not require
not satisfy
not therefore
not to
notable caveat
notably we
nothing forces
notoriously challenging
notoriously hard
numbers 31
numerical programs
numpy tensor
nvidia profiler
nvidia quadro
objective and
objects and
objects conforming
objects on
objects supported
observed to
occur on
of 512
of academic
of all
of any
of automatic
of computation
of computing
of concurrent
of core
of course
of cross
of debugging
of deep
of design
of doing
of domain
of each
of ease
of elements
of engineering
of execution
of feed
of free
of functions
of general
of generative
of gpu
of graph
of high
of imperative
of individual
of keeping
of languages
of large
of libraries
of library
of machine
of many
of mathematical
of memory
of mentions
of models
of new
of of
of open
of performance
of pinned
of progress
of researchers
of scalar
of sending
of serialization
of shuffling
of side
of simplicity
of single
of speed
of speeding
of static
of subsequent
of such
of tasks
of tensors
of these
of tools
of torch
of user
of utilities
of vast
of vibrant
offering of
often composed
often design
often focused
often referred
often various
on another
on arxiv
on disk
on ease
on either
on independent
on mobile
on neural
on one
on python
on reference
on several
on tensors
on them
on these
on top
on usability
on workstation
on write
once and
once this
one idiomatic
one interesting
one needs
one notable
one research
one training
ones at
ones while
only describe
only guarantee
only once
only one
opens the
operate on
operations are
operations of
operations performed
operations usually
operator and
operator invocations
operator making
operator must
operator overloading
operators and
operators can
operators convolution
operators is
operators of
operators to
optimd optim
optimd step
optimg optim
optimg step
optimization is
optimization strategies
optimize the
optimized code
optimized libraries
optimized we
optimizers as
optimizing every
or changing
or if
or more
or operators
or speed
or tensorflow
or using
order to
order will
oriented platform
other commonly
other gpu
other languages
other processes
other python
other than
our choices
our community
our results
our setup
our system
out to
out_sz def
out_sz self
outpaces the
output tensor
output with
outputs than
outside of
overall 43
overall sense
overhead like
overlap the
overloading approach
own multi
own performance
own reference
own specialized
pace of
package and
package popularized
packages for
packages like
paddlepaddle 933
paddlepaddle the
pair the
pandas 20
paper introduces
paper only
paper we
papers each
parallel hardware
parallel primitives
parallel programs
parallel to
parallelism as
parallelism based
parallelization and
parallelizing the
parameter t1
parameter t2
parameters and
parameters are
particular solution
path as
patterns of
peak performance
perfect device
perform forward
performance across
performance additionally
performance characteristics
performance comparable
performance considerations
performance even
performance focused
performance in
performance requirements
performance reusable
performance to
performance wise
performed hence
performs immediate
performs reverse
periodically investigates
periods of
persistence which
philosophy is
pinned cuda
pioneered for
places where
plan to
platform paddlepaddle
platforms it
playing starcraft
plotting and
plotting debugging
point computations
pool of
popular frameworks
popular generative
popular graph
popular in
popular scientific
popular tool
popularized the
possibility to
possible the
possibly lazy
potential benefits
potential neural
power required
practical code
practice kernel
practice pytorch
pragmatic implementation
pragmatic performance
precedes the
preprocessing statistical
preserve the
prevent the
previous ideas
previously allocated
previously queued
primitives all
primitives for
primitives it
primitives or
primitives uses
principles behind
principles that
print statements
prints since
prior work
priorities for
problem by
problem differently
problem the
procedure calls
process an
process are
process communication
process isolation
process print
process therefore
processes and
processes to
processing systems
processing tools
product similarly
production oriented
productive as
productive in
profiler 44
profiler to
program branches
program philosophy
program this
program to
program under
program with
programming language
programming productive
programming style
programs and
programs execute
programs hence
programs that
programs to
programs users
programs we
progress in
project they
proprietary software
provide pragmatic
provided by
provided the
provides automatically
provides the
provides visibility
providing an
providing efficient
providing tools
purpose languages
purpose massively
purpose programming
put researchers
pypy or
python and
python are
python bindings
python classes
python community
python default
python global
python interface
python language
python libraries
python library
python lisp
python made
python note
python own
python program
python starting
python this
python using
pythonic data
pythonic library
pythonic programming
pytorch 1547
pytorch allows
pytorch almost
pytorch an
pytorch as
pytorch because
pytorch builds
pytorch caching
pytorch could
pytorch easily
pytorch for
pytorch implements
pytorch in
pytorch jit
pytorch library
pytorch maintains
pytorch model
pytorch most
pytorch must
pytorch needs
pytorch on
pytorch operators
pytorch performs
pytorch python
pytorch should
pytorch simple
pytorch solved
pytorch strives
pytorch success
pytorch such
pytorch tensors
pytorch that
pytorch tracks
pytorch uses
pytorch with
quadro gp100
quantify how
quantifying the
queue cuda
queue various
queued work
queues the
quickly address
quickly create
quickly outpaces
quite dramatically
randn in_sz
randn out_sz
range of
rapidly from
ratio depends
reach peak
reaches zero
real_sample real_label
reallocate memory
really be
really complicated
reassigns it
received pytorch
recent years
recognized the
recognizing individual
record timeline
recursive functions
reduce style
references internal
references made
referred to
reflected in
region which
regular python
regular threaded
relative performance
release of
released exactly
releasing brand
relies on
relu t1
rely on
remaining efficient
remote procedure
repeatedly to
replace any
replacement for
report the
repository of
representation of
representative timeline
represented as
represents the
reproduce our
require holding
required by
requirements of
research community
research hence
researchers first
researchers while
resembles regular
resolution of
resort to
resource that
resources and
respect to
restructure the
result of
result to
return nn
return self
reusable deep
reusing previously
reverse mode
rewrite their
rich ecosystem
rich offering
rigid apis
rounds up
row depicts
row shows
run approach
run either
run framework
run it
running at
running deep
running on
running optimizers
running their
runs ahead
runtime as
runtime enables
runtime making
runtime periodically
rust bindings
sacrificing performance
safety we
same form
same order
same stream
same time
same version
samples per
satisfy those
saturate the
saved by
scalability however
scalability of
scalar output
scale images
scarce resource
scarcity of
scheduling is
scheduling the
scheme to
scientific community
scientists are
scipy 19
scripting languages
search case
second the
section we
seems limiting
self activations
self in_sz
self return
self t3
semantics have
sending it
sense of
sent to
separate control
separate models
separately libraries
separation between
sequence of
sequences of
serialization used
serialize execution
setting as
setup but
several common
several other
share the
shared memory
sharing of
short amount
should really
show an
shows representative
shows that
shows the
shuffling batching
side effect
side effects
sides only
sidestep this
significant amount
significantly easier
significantly from
similar functionality
similar mechanism
similarly models
similarly new
simple and
simple can
simple design
simple it
simple sequences
simpler to
simplicity and
simplified training
simplifies the
simultaneously empowering
since 2014
since gradient
since pytorch
since streams
single machine
single training
situations and
sizes is
skill to
slightly incomplete
slow down
so either
so on
so the
so while
softmax t3
softmax to
software for
software fostered
software movement
software such
solution ensures
solution than
solved the
some recent
source differentiation
source python
source to
specialized libraries
specialized memory
specific languages
specific memory
specifically to
specified by
specify the
speed but
speeding up
sprinkle the
standard debuggers
standard multiprocessing
standard plotting
starcraft 28
starcraft 36
start by
start running
starting in
starting with
starts reusing
state again
state of
stateful functions
statements standard
static dataflow
statistical analysis
stems from
step real_sample
steps needed
still allowing
stream and
stream as
stream but
stream work
streams it
streams serialize
strict separation
strives to
structure the
structure their
struggle with
style high
style primitives
subclass of
subclassing torch
submissions mentioning
subsequent iterations
subsequent ones
subsystems as
subtle and
success stems
such an
suite of
summarized in
super __init__
support arbitrary
support the
support this
supported by
supports code
surprisingly short
susceptible to
swift but
synchronization to
synchronization would
synchronize gradients
system allows
system for
system to
systems are
systems neurips
t1 torch
t2 def
t2 nn
t2 torch
t3 self
table on
table training
take constant
take significant
take the
takes around
takes different
takes great
taking up
task on
tasks all
technique for
technique of
tensor allocator
tensor and
tensor computations
tensor data
tensor memory
tensor method
tensor operations
tensor operators
tensor to
tensorflow 1422
tensorflow to
tensors and
tensors become
tensors into
tensors sent
tensors using
than comprehensive
than cpu
than inputs
than python
that allows
that any
that balances
that can
that combine
that compose
that depend
that do
that does
that drove
that ecosystem
that either
that enabled
that every
that implements
that memory
that no
that nothing
that only
that operate
that performance
that pytorch
that represents
that shows
that supports
that they
that users
that way
that would
the 1960s
the ability
the advent
the allocations
the appendix
the arrows
the autograd
the automatic
the availability
the average
the basic
the behavior
the benchmarks
the bottom
the built
the careful
the colored
the commonly
the communication
the complexity
the computed
the computing
the construction
the converted
the corresponding
the costs
the critical
the cudafree
the cudnn
the dataloader
the daunting
the deallocation
the deep
the desired
the device
the discriminator
the dlpack
the dynamic
the efficiency
the evaluation
the exact
the existing
the expense
the experimentation
the fact
the familiar
the fast
the features
the field
the floating
the freed
the full
the function
the game
the garbage
the generator
the gnmtv2
the gpus
the gray
the imperative
the increased
the incremental
the indexing
the initial
the internal
the key
the latest
the length
the libtorch
the machine
the memory
the models
the monthly
the ncf
the needs
the network
the neural
the new
the numerical
the nvidia
the open
the openness
the operations
the operator
the operators
the overall
the potential
the primitives
the principles
the problem
the reallocation
the really
the research
the resnet
the resolution
the result
the rich
the runtime
the scarcity
the scientific
the search
the simple
the specific
the state
the steps
the system
the torch
the torchscript
the training
the types
the typical
the underlying
the use
the utilization
the validity
the value
the vectorjacobian
the very
the whole
the word
theano are
theano construct
their impact
their models
their parameters
their programs
their project
their python
them cooperatively
them hence
them know
them separately
them that
them the
themselves evolved
then allocated
then be
theoretically be
there are
there has
there is
therefore critical
therefore it
therefore its
therefore to
these deep
these hardware
these overheads
these tools
these trends
these two
these work
they implement
they likely
things it
third with
this allocator
this automatically
this bottleneck
this combination
this core
this count
this define
this effect
this ensures
this everything
this example
this exchange
this facilitates
this fulfilled
this growing
this interface
this lets
this made
this mechanism
this path
this result
this section
this setting
this setup
this solution
this system
this task
this technique
this to
this use
those can
those criteria
those operations
those operators
thread communication
thread for
threaded programs
threads is
threads to
three popular
three times
through code
throughout the
throughput higher
throughput is
time and
time deep
time hence
time it
time machine
time no
time rigid
time saved
time source
time this
time would
times in
times longer
to account
to add
to all
to alleviate
to also
to as
to asynchronously
to automatically
to batches
to certain
to classify
to continue
to continuing
to convert
to create
to custom
to deep
to do
to exchange
to experiment
to fully
to further
to have
to highlight
to hold
to implementations
to impose
to instrument
to interpret
to jumpstart
to just
to later
to leverage
to machine
to maintain
to make
to manipulate
to manually
to measure
to monolithic
to multiple
to multiples
to multivariate
to new
to not
to optimize
to other
to overlap
to perform
to preserve
to quantify
to quickly
to replace
to reproduce
to restructure
to rewrite
to shared
to source
to specify
to sprinkle
to structure
to three
to trace
to track
to treat
to utilize
to wait
to work
to write
together to
tokens per
tool in
tools including
tools like
tools many
tools mentioned
tools offload
tools pytorch
top of
top priorities
top row
torch autograd
torch dynet
torch eigen
torch from_numpy
torch jax
torch mm
torch multiprocessing
torch utils
torch7 25
torch7 utilized
torchscript engine
towards the
trace of
trace the
traces of
track the
track their
tracks both
tradeoff is
trading 10
training and
training iteration
training of
training process
training speed
training step
training techniques
transparently handles
treat memory
trends and
trends by
trends in
tried to
triggers to
tuned for
turned multidimensional
turned out
two events
two goals
two intel
two loss
two methods
two separate
types of
typical way
typically expressed
unacceptable in
uncommon feature
under the
underlying memory
understand how
unique feature
unless they
until all
unwanted behaviors
up all
up allocations
up cache
up fragmented
up representation
up the
up to
up with
update discriminator
update generator
us saturate
us state
us track
usability centric
usability or
usability with
usage patterns
use case
use ease
use is
use model
use more
use on
use the
use there
use trading
used as
used deep
used for
used objects
used the
used to
useful pytorch
user defined
user error
user from
user programs
user to
user unless
user we
users and
users do
users in
users leverage
users often
users was
uses multiple
uses of
uses the
using 32bit
using all
using array
using forwardmode
using less
using yaml
usually beneficial
usually represented
usually resort
usually take
utilities are
utilities that
utilization in
utilization of
utilize all
utilize reference
utilize techniques
utilized the
utilizing other
utils data
v4 cpus
validity of
value of
vancouver canada
varied numerical
various benchmarks
various machine
various operators
vast repository
vectorjacobian product
version of
versioning system
via automatic
vibrant communities
visibility into
visualization tools
vital to
wait for
want to
was last
was pioneered
was to
was tuned
way listing
way of
way that
way to
we always
we are
we attribute
we chose
we compare
we could
we counted
we demonstrate
we detail
we emphasize
we expect
we have
we plan
we report
we show
we start
we tried
we use
we used
weaker resulting
weaving previous
well the
well this
were performed
when dealing
when tensors
where python
where they
whether its
which builds
which can
which computes
which does
which in
which incrementally
which limits
which more
which queues
which specify
which takes
while allowing
while maintaining
while most
while remaining
while still
while there
while this
while we
whole computation
whole design
whose forward
will be
will empower
will have
will occur
wise it
with automatic
with body
with eager
with explicit
with external
with fairly
with for
with implicit
with large
with lush
with more
with one
with other
with packages
with python
with pytorch
with respect
with several
with standard
with tensor
with this
with two
without any
without further
without knowing
without sacrificing
word pytorch
work as
work done
work has
work is
work on
work queue
work quickly
work such
work together
workflows defining
working on
works and
workstation with
worse is
would let
would negate
would struggle
write cuda
write to
writers usually
writing models
written in
xeon e5
yaml meta
years there
zero note
19 resnet
advantage of
adversarial networks
ahead of
alexnet vgg
all the
almost never
amount of
an essential
an imperative
and can
and common
and data
and ease
and flexibility
and frees
and its
and lua
and more
and pytorch
and the
and those
are all
array based
arrays and
as caffe
as gpus
as python
as shown
as well
aspect of
asynchronously on
at the
based optimization
be easily
be handled
be performed
be used
because it
bindings to
builds up
built in
by deferring
by providing
by run
by the
chainer and
code executed
compelling performance
composed of
computation to
cost of
cpu and
cpu the
cuda kernels
cuda memory
cuda stream
data flow
data loaders
data loading
debugging and
def forward
deferring the
define by
designed to
development of
differentiation system
discriminator fake
do not
does not
dynamic eager
each tensor
eager execution
ease of
easy and
ensure that
ensures that
every aspect
example it
execute operators
execution for
feature of
first class
first few
flexibility of
for data
for deep
for example
for instance
for model
forward self
frameworks such
free software
freed on
function and
functions with
general purpose
generative adversarial
global interpreter
gpu because
gpu memory
gpu the
gpu this
gradient based
handle this
hard to
hardware accelerators
have to
high performance
host cpu
however the
implement their
implementation and
implementation of
in deep
in fact
in its
in practice
in this
in_sz out_sz
instance the
integrated in
interoperability and
interpreter lock
is also
is better
is executed
is handled
is not
is notoriously
is one
is that
it also
it provides
it to
its execution
kernels that
languages such
learning in
learning library
learning should
learning tools
lets us
libraries for
library and
library that
libtorch library
lock 33
loops and
loss discriminator
lua and
lush 14
made it
mechanism to
memory is
memory management
model and
model is
model the
models are
most of
multiprocessing module
needs to
neural network
neural networks
nn functional
nn parameter
note that
number of
numpy 12
numpy arrays
of cuda
of data
of dynamic
of its
of our
of python
of resnet
of that
of their
of this
of those
of use
on all
on both
on cpu
one of
one pool
one stream
open source
operators asynchronously
operators on
optim adam
optimizers and
or on
our users
over the
part of
per second
per stream
percentage of
performance and
performance cliffs
performance of
performance this
performed on
pool per
possible to
programming model
provides an
python code
python ecosystem
python interpreter
python is
python packages
python programs
pytorch by
pytorch can
pytorch extends
pytorch has
pytorch is
pytorch programs
pytorch takes
pytorch to
reference counting
result in
resulting in
results are
scientific computing
second for
self conv
self fc
self nn
set of
should be
shown in
simple but
since the
speed and
speed for
speed of
stream design
support for
synchronization is
system is
t1 self
take advantage
techniques like
tensorflow and
tensors which
that allow
that it
that mention
that of
that the
that these
that this
that we
the allocator
the computation
the control
the cost
the cuda
the data
the development
the fastest
the first
the free
the future
the global
the gradient
the host
the implementation
the library
the number
the one
the performance
the process
the program
the pytorch
the same
the speed
the tensor
the top
the user
the users
the work
their code
their own
them to
they are
they can
this allows
this approach
this design
this paper
this problem
those that
timeline of
to achieve
to avoid
to deliver
to easily
to ensure
to execute
to find
to handle
to implement
to improve
to manage
to queue
to support
to take
to this
to use
tools that
torch randn
us to
use of
used in
used on
users are
users can
users to
using the
very popular
vgg 19
we also
we can
well as
which is
which lets
with careful
with the
as the
automatic differentiation
can be
execution of
for the
in the
it is
learning frameworks
machine learning
of pytorch
of time
on gpu
on the
resnet 50
such as
the cpu
the execution
the gpu
the python
to be
to the
deep learning
of the
